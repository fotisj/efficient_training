07/27/2020 11:06:15 - INFO - transformers.trainer -   ***** Running training *****
07/27/2020 11:06:15 - INFO - transformers.trainer -     Num examples = 8030
07/27/2020 11:06:15 - INFO - transformers.trainer -     Num Epochs = 5
07/27/2020 11:06:15 - INFO - transformers.trainer -     Instantaneous batch size per device = 32
07/27/2020 11:06:15 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 32
07/27/2020 11:06:15 - INFO - transformers.trainer -     Gradient Accumulation steps = 1
07/27/2020 11:06:15 - INFO - transformers.trainer -     Total optimization steps = 1255
07/27/2020 11:06:15 - INFO - transformers.trainer -     Starting fine-tuning.
Epoch:   0%|                                                                                         | 0/5 [00:00<?, ?it/s]
Iteration:   0%|                                                                                   | 0/251 [00:00<?, ?it/s][A
Iteration:   0%|â–Ž                                                                          | 1/251 [00:00<03:31,  1.18it/s][AIteration:   0%|â–Ž                                                                          | 1/251 [00:01<04:12,  1.01s/it]
Epoch:   0%|                                                                                         | 0/5 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "run_ner.py", line 304, in <module>
    main()
  File "run_ner.py", line 229, in main
    model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None
  File "/home/konle/anaconda3/envs/deep/lib/python3.6/site-packages/transformers/trainer.py", line 499, in train
    tr_loss += self._training_step(model, inputs, optimizer)
  File "/home/konle/anaconda3/envs/deep/lib/python3.6/site-packages/transformers/trainer.py", line 637, in _training_step
    loss.backward()
  File "/home/konle/anaconda3/envs/deep/lib/python3.6/site-packages/torch/tensor.py", line 195, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/konle/anaconda3/envs/deep/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 10.73 GiB total capacity; 4.96 GiB already allocated; 27.69 MiB free; 5.02 GiB reserved in total by PyTorch)
