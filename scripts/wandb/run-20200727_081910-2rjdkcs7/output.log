07/27/2020 10:19:11 - INFO - transformers.trainer -   ***** Running training *****
07/27/2020 10:19:11 - INFO - transformers.trainer -     Num examples = 1494
07/27/2020 10:19:11 - INFO - transformers.trainer -     Num Epochs = 5
07/27/2020 10:19:11 - INFO - transformers.trainer -     Instantaneous batch size per device = 32
07/27/2020 10:19:11 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 32
07/27/2020 10:19:11 - INFO - transformers.trainer -     Gradient Accumulation steps = 1
07/27/2020 10:19:11 - INFO - transformers.trainer -     Total optimization steps = 235
07/27/2020 10:19:11 - INFO - transformers.trainer -     Starting fine-tuning.
Epoch:   0%|                                                                                         | 0/5 [00:00<?, ?it/s]
Iteration:   0%|                                                                                    | 0/47 [00:00<?, ?it/s][A
Iteration:   2%|█▌                                                                          | 1/47 [00:00<00:39,  1.17it/s][A
Iteration:   4%|███▏                                                                        | 2/47 [00:01<00:31,  1.42it/s][AIteration:   4%|███▏                                                                        | 2/47 [00:01<00:28,  1.56it/s]
Epoch:   0%|                                                                                         | 0/5 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "run_ner.py", line 304, in <module>
    main()
  File "run_ner.py", line 229, in main
    model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None
  File "/home/konle/anaconda3/envs/deep/lib/python3.6/site-packages/transformers/trainer.py", line 499, in train
    tr_loss += self._training_step(model, inputs, optimizer)
  File "/home/konle/anaconda3/envs/deep/lib/python3.6/site-packages/transformers/trainer.py", line 637, in _training_step
    loss.backward()
  File "/home/konle/anaconda3/envs/deep/lib/python3.6/site-packages/torch/tensor.py", line 195, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/konle/anaconda3/envs/deep/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 10.73 GiB total capacity; 2.66 GiB already allocated; 9.69 MiB free; 2.76 GiB reserved in total by PyTorch)
