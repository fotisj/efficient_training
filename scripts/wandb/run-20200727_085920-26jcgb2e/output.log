07/27/2020 10:59:21 - INFO - transformers.trainer -   ***** Running training *****
07/27/2020 10:59:21 - INFO - transformers.trainer -     Num examples = 8037
07/27/2020 10:59:21 - INFO - transformers.trainer -     Num Epochs = 5
07/27/2020 10:59:21 - INFO - transformers.trainer -     Instantaneous batch size per device = 32
07/27/2020 10:59:21 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 32
07/27/2020 10:59:21 - INFO - transformers.trainer -     Gradient Accumulation steps = 1
07/27/2020 10:59:21 - INFO - transformers.trainer -     Total optimization steps = 1260
07/27/2020 10:59:21 - INFO - transformers.trainer -     Starting fine-tuning.
Epoch:   0%|                                                                                         | 0/5 [00:00<?, ?it/s]
Iteration:   0%|                                                                                   | 0/252 [00:00<?, ?it/s][AIteration:   0%|                                                                                   | 0/252 [00:00<?, ?it/s]
Epoch:   0%|                                                                                         | 0/5 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "run_ner.py", line 304, in <module>
    main()
  File "run_ner.py", line 229, in main
    model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None
  File "/home/konle/anaconda3/envs/deep/lib/python3.6/site-packages/transformers/trainer.py", line 499, in train
    tr_loss += self._training_step(model, inputs, optimizer)
  File "/home/konle/anaconda3/envs/deep/lib/python3.6/site-packages/transformers/trainer.py", line 637, in _training_step
    loss.backward()
  File "/home/konle/anaconda3/envs/deep/lib/python3.6/site-packages/torch/tensor.py", line 195, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/konle/anaconda3/envs/deep/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 10.73 GiB total capacity; 3.68 GiB already allocated; 24.69 MiB free; 3.72 GiB reserved in total by PyTorch)
